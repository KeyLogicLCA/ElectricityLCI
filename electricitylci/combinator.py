#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# combinator.py
#
##############################################################################
# REQUIRED MODULES
##############################################################################
import logging

import pandas as pd

from electricitylci.globals import output_dir
from electricitylci.model_config import model_specs
from electricitylci.eia860_facilities import eia860_balancing_authority
from electricitylci.generation import add_temporal_correlation_score
from electricitylci.utils import read_ba_codes
import fedelemflowlist as fedefl


##############################################################################
# MODULE DOCUMENTATION
##############################################################################
__doc__ = """This module contains several utility methods for combining
data from different sources, such as power plant-level emissions
provided by Standardized Emission and Waste Inventories (StEWI), NETL
fuel emissions (e.g., coal mining/transport, natural gas extraction,
nuclear fuel cycle), and maps emissions based on the Federal LCA Commons
Elementary Flow List in order to provide life cycle inventory.

Last edited:
    2025-02-05
"""
__all__ = [
    "BA_CODES",
    "add_fuel_inputs",
    "concat_map_upstream_databases",
    "concat_clean_upstream_and_plant",
    "fill_nans",
]


##############################################################################
# GLOBALS
##############################################################################
BA_CODES = read_ba_codes()
'''pandas.DataFrame : Balancing authority, FERC, and EIA region data'''


##############################################################################
# FUNCTIONS
##############################################################################

def add_fuel_inputs(gen_df, upstream_df, upstream_dict):
    """Convert the upstream emissions database to fuel inputs and add them
    to the generator data frame.

    This is in preparation of generating unit processes for openLCA.

    Parameters
    ----------
    gen_df : pandas.DataFrame
        The generator data frame containing power plant emissions (e.g., from
        :module:`generation`.\\ :func:`create_generation_process_df`).
    upstream_df : pandas.DataFrame
        The combined upstream data frame (e.g., from
        :func:`get_upstream_process_df`).
    upstream_dict : dict
        This is the dictionary of upstream "unit processes" as generated by
        :func:`write_upstream_process_database_to_dict` after upstream_dict
        has been written to JSON-LD (e.g., with
        :func:`write_process_dicts_to_jsonld`).
        This is important because the UUIDs for the upstream "unit processes"
        are only generated when written to JSON-LD.

    Returns
    -------
    pandas.DataFrame

    Examples
    --------
    >>> from electricitylci import get_gen_plus_netl
    >>> from electricitylci import get_upstream_process_df
    >>> from electricitylci import write_upstream_process_database_to_dict
    >>> from electricitylci import write_process_dicts_to_jsonld
    >>> generation_df = get_gen_plus_netl()
    >>> ups_df = get_upstream_process_df(2020)
    >>> ups_dict = write_upstream_process_database_to_dict(ups_df)
    >>> ups_dict = write_process_dicts_to_jsonld(ups_dict)
    >>> gen_plus_fuel = add_fuel_inputs(generation_df, ups_df, ups_dict)
    """
    upstream_reduced = upstream_df.drop_duplicates(
        subset=["eGRID_ID", "stage_code", "quantity"]
    )
    fuel_df = pd.DataFrame(columns=gen_df.columns)

    # The upstream reduced should only have one instance of each plant/stage
    # code combination. We'll first map the upstream dictionary to each plant
    # and then expand that dictionary into columns we can use. The goal is
    # to generate the fuels and associated metadata with each plant. That will
    # then be merged with the generation database.
    fuel_df["flowdict"] = upstream_reduced["stage_code"].map(upstream_dict)
    expand_fuel_df = fuel_df["flowdict"].apply(pd.Series)
    fuel_df.drop(columns=["flowdict"], inplace=True)

    # Fill in with appropriate fields.
    # NOTE: 'quantity' is units of Electricity (MWh) for construction and
    # nameplate capacity (MW) for coal, heat input (MJ) for petroleum, tons
    # for coal mining, etc.
    fuel_df["Compartment"] = "input"
    fuel_df["FlowName"] = expand_fuel_df["q_reference_name"]
    fuel_df["stage_code"] = upstream_reduced["stage_code"]
    fuel_df["FlowAmount"] = upstream_reduced["quantity"]
    fuel_df["FlowUUID"] = expand_fuel_df["q_reference_id"]
    fuel_df["Unit"] = expand_fuel_df["q_reference_unit"]
    fuel_df["eGRID_ID"] = upstream_reduced["eGRID_ID"]
    fuel_df["FacilityID"] = upstream_reduced["eGRID_ID"]
    fuel_df["FuelCategory"] = upstream_reduced["FuelCategory"]
    fuel_df["Year"] = upstream_reduced["Year"]
    fuel_df["Source"] = upstream_reduced["Source"]

    # Drop the merge columns from fuel data frame
    merge_cols = [
        "Age",
        "Balancing Authority Code",
        "Balancing Authority Name",
        "Electricity",
        "NERC",
        "Subregion",
    ]
    merge_cols = [x for x in merge_cols if x in fuel_df.columns]
    fuel_df.drop(columns=merge_cols, inplace=True)

    # Get location data for each facility and add it to fuel data frame.
    # NOTE: some facilities may not have location data.
    gen_df_reduced = gen_df[merge_cols + ["eGRID_ID"]].drop_duplicates(
        subset=["eGRID_ID"]
    )
    fuel_df = fuel_df.merge(
        right=gen_df_reduced,
        left_on="eGRID_ID",
        right_on="eGRID_ID",
        how="left",
    )

    # Drop rows that didn't link up.
    fuel_df.dropna(subset=["Electricity"], inplace=True)

    # Add data quality indicators and elementary flow prime context (inputs)
    fuel_df["TemporalCorrelation"] = add_temporal_correlation_score(
        fuel_df["Year"], model_specs.electricity_lci_target_year)
    fuel_df["DataCollection"] = 5
    fuel_df["GeographicalCorrelation"] = 1
    fuel_df["TechnologicalCorrelation"] = 1
    fuel_df["DataReliability"] = 1
    fuel_df["ElementaryFlowPrimeContext"] = "input"

    # Create a series mapping facility IDs to their primary fuel categories
    fuel_cat_key = (
        gen_df[["FacilityID", "FuelCategory"]].drop_duplicates(
            subset="FacilityID").set_index("FacilityID")
    )
    fuel_df["FuelCategory"] = fuel_df["FacilityID"].map(
        fuel_cat_key["FuelCategory"]
    )

    # Concatenate the generation processes with their upstream processes.
    gen_plus_up_df = pd.concat([gen_df, fuel_df], ignore_index=True)
    #gen_plus_up_df = remove_mismatched_inventories(gen_plus_up_df)
    gen_plus_up_df = fill_nans(gen_plus_up_df, model_specs.eia_gen_year)

    # Taking out anything with New Brunswick System Operator so that
    # these fuel inputs (for a very small US portion of NBSO) don't get mapped
    # to the Canadian import rollup (i.e., double-counted)
    _ba_col = "Balancing Authority Name"
    _ba_name = "New Brunswick System Operator"
    gen_plus_up_df = gen_plus_up_df.loc[
        gen_plus_up_df[_ba_col] != _ba_name, :].reset_index(drop=True)
    return gen_plus_up_df


def concat_map_upstream_databases(eia_gen_year, *arg, **kwargs):
    """Concatenate and map all of the databases given as args.

    All of the emissions in the combined database are mapped to the
    federal elementary flows list based on the mapping file 'eLCI' in
    preparation for being turned into openLCA processes and combined with
    the generation emissions. Unmapped emissions are dropped from the
    inventory.

    All resource flows are preserved.

    Parameters
    ----------
    eia_gen_year : int
        Becomes the 'Year' column in the returned data frame.
    *arg : tuple
        Tuple of pandas.DataFrame objects to be combined, generated by the
        upstream modules or renewables modules (e.g., .nuclear_upstream,
        .petroleum_upstream, and .solar_upstream).
    **kwargs : dict
        A dictionary of named arguments. The key 'group_name' triggers a
        tuple to be returned, rather than just a data frame (see notes).

    Returns
    -------
    pandas.DataFrame or tuple
        The data frame contains the columns: 'plant_id', 'FuelCategory','stage_code', 'FlowName', 'Compartment', 'Compartment_path', 'FlowUUID', 'Unit', 'ElementaryFlowPrimeContext', 'FlowAmount', 'quantity', 'Source', 'Year', 'Electricity', and 'input'.

    Notes
    -----
    If 'group_name' is provided in kwargs, then the function will return a
    tuple containing the mapped data frame and lists of tuples for the unique
    mapped and unmapped flows, as well as write the results to text file.

    For EIA generation year 2016, there is a reported 2375 unmatched and
    2036 matched flows for renewable energy power plants.

    This method sets all data vintages with EIA generation year.

    Examples
    --------
    >>> import electricitylci.geothermal as geo
    >>> import electricitylci.solar_upstream as solar
    >>> import electricitylci.wind_upstream as wind
    >>> import electricitylci.solar_thermal_upstream as solartherm
    >>> eia_gen_year = config.model_specs.eia_gen_year
    >>> geo_df = geo.generate_upstream_geo(eia_gen_year)
    >>> solar_df = solar.generate_upstream_solar(eia_gen_year)
    >>> wind_df = wind.generate_upstream_wind(eia_gen_year)
    >>> solartherm_df = solartherm.generate_upstream_solarthermal(eia_gen_year)
    >>> netl_gen, u_list, m_list = concat_map_upstream_databases(
    ...    eia_gen_year, geo_df, solar_df, wind_df, solartherm_df,
    ...    group_name='renewable')
    """
    mapped_column_dict = {
        "TargetFlowName": "FlowName",
        "TargetFlowUUID": "FlowUUID",
        "TargetFlowContext": "Compartment",
        "TargetUnit": "Unit",
    }
    logging.info(
        f"Concatenating and flow-mapping {len(arg)} upstream databases.")
    upstream_df_list = list()
    # HOTFIX: index out data frames from tuple before mutate [2023-12-21; TWD]
    for i in range(len(arg)):
        df = arg[i]
        if isinstance(df, pd.DataFrame):
            if "Compartment_path" not in df.columns:
                # HOTFIX: allow for resources [241011; TWD]
                df = map_compartment_path(df)
            upstream_df_list.append(df)
    upstream_df = pd.concat(upstream_df_list, ignore_index=True, sort=False)
    # Hoping to reduce memory usage or at least make more of it available
    # for the later groupby.
    del(arg)
    # See https://github.com/USEPA/fedelemflowlist
    # The mapping data includes a conversion factor to convert everything into
    # standard units (e.g., kg, MJ, m2*a). Note that 'SourceFlowContext' is
    # already in lowercase letters, which is why no change happens below.
    logging.info("Creating flow mapping database")
    flow_mapping = fedefl.get_flowmapping('eLCI')

    # as hotfix for https://github.com/USEPA/ElectricityLCI/issues/274
    # append full flowlist to the flow mapping file (dropping duplicates)
    # to catch any other mappings of flows that use the same name as already
    # in the flow list
    flowlist = (fedefl.get_flows()
                .filter(['Flowable', 'Context', 'Unit', 'Flow UUID'])
                .assign(SourceFlowName = lambda x: x['Flowable'])
                .assign(SourceFlowContext = lambda x: x['Context'])
                .assign(SourceUnit = lambda x: x['Unit'])
                .assign(ConversionFactor = 1.0)
                .rename(columns={'Flowable': 'TargetFlowName',
                                 'Flow UUID': 'TargetFlowUUID',
                                 'Unit': 'TargetUnit',
                                 'Context': 'TargetFlowContext'})
                )
    flow_mapping = (pd.concat([flow_mapping, flowlist], ignore_index=True)
                    .drop_duplicates(
                        subset=['SourceFlowName', 'SourceFlowContext', 'TargetFlowName'])
                    )
    flow_mapping["SourceFlowName"] = flow_mapping["SourceFlowName"].str.lower()

    logging.info("Preparing upstream df for merge")
    upstream_df["FlowName_orig"] = upstream_df["FlowName"]
    upstream_df["Compartment_orig"] = upstream_df["Compartment"]
    upstream_df["Compartment_path_orig"] = upstream_df["Compartment_path"]
    upstream_df["Unit_orig"] = upstream_df["Unit"]
    upstream_df["FlowName"] = upstream_df["FlowName"].str.lower().str.rstrip()
    upstream_df["Compartment"] = (
        upstream_df["Compartment"].str.lower().str.rstrip()
    )
    upstream_df["Compartment_path"] = (
        upstream_df["Compartment_path"].str.lower().str.rstrip()
    )
    # HOTFIX: new pandas syntax [TWD; 2024-02-26]
    upstream_df.fillna({"Unit": "<blank>"}, inplace=True)
    upstream_columns = upstream_df.columns

    logging.info("Grouping upstream database")
    groupby_cols = [
        "fuel_type",
        "stage_code",
        "FlowName",
        "Compartment",
        "input",
        "plant_id",
        "Compartment_path",
        "Unit",
        "FlowName_orig",
        "Compartment_path_orig",
        "Unit_orig",
        "Source"
    ]
    # Addings years to the groupby when data is passed to the function
    # that includes years.
    if "Year" in upstream_df.columns:
        groupby_cols=groupby_cols+["Year"]
    # Ensure flow amounts are floats
    upstream_df["FlowAmount"] = upstream_df["FlowAmount"].astype(float)
    # Refactoring this a bit due to possibility of some columns not being present.
    # Also gets rid of an if statement!
    possible_quant_columns = [
        "FlowAmount",
        "quantity",
        "Electricity",
        # Issue #296 Adding the DQI categories so that they aren't lost
        # I think mean is an okay operation here because given how
        # the groupby is done, I don't believe there will be any variation
        # in these values. If I did think so, a weighted mean might be
        # more appropriate (and also more computationally intensive).
        # It might also be appropriate to even just use a first-value
        # approach.
        "DataCollection",
        "TemporalCorrelation",
        "GeographicalCorrelation",
        "TechnologicalCorrelation",
        "DataReliability"
    ]
    actual_quant_columns = [
        x for x in possible_quant_columns if x in upstream_df.columns
    ]
    aggregation_dictionary = {
        "FlowAmount": "sum",
        "quantity": "mean",
        "Electricity": "mean",
        "DataCollection": "mean",
        "TemporalCorrelation": "mean",
        "GeographicalCorrelation": "mean",
        "TechnologicalCorrelation": "mean",
        "DataReliability": "mean"
    }
    actual_aggregation_dictionary = {
        x: aggregation_dictionary[x] for x in actual_quant_columns
    }
    upstream_df_grp = upstream_df.groupby(
        groupby_cols, as_index=False, dropna=False
        ).agg(actual_aggregation_dictionary)
    grp_columns = groupby_cols + actual_quant_columns
    # Trying to reduce memory usage. There's a section below that will re-use
    # upstream_df if kwargs are provided. If kwargs is empty, we can delete
    # this and hopefully save some memory.
    if len(kwargs)==0:
        del(upstream_df)
    logging.info("Merging upstream database and flow mapping")
    # Hoping to save some memory by only mapping the index of the flow_mapping
    # dataframe to a slice of upstream_df_grp, so that we only copy a small
    # dataframe during the merge. Then we can used the matched indeces to assign
    # column values later on.
    flowmapping_small = (
        flow_mapping[["SourceFlowName", "SourceFlowContext"]]
        .reset_index()
        .rename(columns={"index": "orig_fm_index"})
    )
    upstream_df_grp_small = (
        upstream_df_grp[["FlowName", "Compartment_path"]]
        .reset_index()
        .rename(columns={"index": "orig_up_index"})
    )
    upstream_mapped_df = pd.merge(
        left=upstream_df_grp_small,
        right=flowmapping_small,
        left_on=["FlowName", "Compartment_path"],
        right_on=["SourceFlowName", "SourceFlowContext"],
        how="left",
    )
    for col in [
        "TargetFlowContext",
        "TargetFlowName",
        "TargetUnit",
        "ConversionFactor",
        "TargetFlowUUID"
    ]:
        upstream_mapped_df[col] = pd.NA
        upstream_mapped_df.loc[
            ~upstream_mapped_df["orig_fm_index"].isna(), col
        ] = flow_mapping.loc[
            upstream_mapped_df.loc[
                ~upstream_mapped_df["orig_fm_index"].isna(), "orig_fm_index"
            ].values,
            col,
        ].values
    for col in grp_columns:
        upstream_mapped_df[col]=pd.NA
        upstream_mapped_df.loc[
            ~upstream_mapped_df["orig_up_index"].isna(), col
        ] = upstream_df_grp.loc[
            upstream_mapped_df.loc[
                ~upstream_mapped_df["orig_up_index"].isna(), "orig_up_index"
            ].values,
            col,
        ].values
    # The new piecewise merging is causing quantitative
    # columns to be converted to objects, so undoing that here
    for col in actual_quant_columns:
        upstream_mapped_df[col]=upstream_mapped_df[col].astype(float)
    
    # Preserve unmapped resource flows;
    #   copy over the flow name, compartment and units and
    #   set conversion factor equal to 1.0.
    #   Note that all other unmapped flows are lost, which is based on
    #   only keeping emissions that contribute to TRACI impacts;
    #   See eLCI.csv here: https://github.com/USEPA/fedelemflowlist
    r_flows = (upstream_mapped_df['TargetFlowContext'].isna()) & (
        upstream_mapped_df['input']
    )
    upstream_mapped_df.loc[
        r_flows,
        'TargetFlowName'] = upstream_mapped_df.loc[r_flows, 'FlowName_orig']
    upstream_mapped_df.loc[
        r_flows,
        'TargetFlowContext'] = upstream_mapped_df.loc[r_flows,
                                                      'Compartment_path_orig']
    upstream_mapped_df.loc[
        r_flows,
        'TargetUnit'] = upstream_mapped_df.loc[r_flows, 'Unit_orig']
    upstream_mapped_df.loc[r_flows, 'ConversionFactor'] = 1.0

    # Substitute FEDEFL mapped names, compartments, and units
    upstream_mapped_df.drop(
        columns={"FlowName", "Compartment", "Unit"},
        inplace=True
    )
    upstream_mapped_df = upstream_mapped_df.rename(
        columns=mapped_column_dict,
        copy=False
    )
    upstream_mapped_df.drop_duplicates(
        subset=["plant_id", "FlowName", "Compartment_path", "FlowAmount"],
        inplace=True,
    )
    upstream_mapped_df.dropna(subset=["FlowName"], inplace=True)

    logging.info("Applying conversion factors")
    upstream_mapped_df["FlowAmount"] = (
        upstream_mapped_df["FlowAmount"]
        * upstream_mapped_df["ConversionFactor"]
    )

    # Set the fuel category (all caps)
    upstream_mapped_df.rename(
        columns={"fuel_type": "FuelCategory"}, inplace=True
    )
    upstream_mapped_df["FuelCategory"] = upstream_mapped_df[
        "FuelCategory"
    ].str.upper()

    # Set the Elementary flow prime context
    upstream_mapped_df["ElementaryFlowPrimeContext"] = "emission"
    upstream_mapped_df.loc[
        upstream_mapped_df["input"],
        "ElementaryFlowPrimeContext"] = "resource"

    # WARNING: don't use with HYDRO, which has its own data year
    # 3/14/25 MBJ Ran into issue where this was overwriting years that were
    # defined upstream (renewable O&M). I think in almost all cases year
    # will already be defined now. But just in case...
    if "Year" not in upstream_mapped_df.columns:
        upstream_mapped_df["Year"]=pd.NA
    upstream_mapped_df.loc[upstream_mapped_df["Year"].isna(),"Year"] = eia_gen_year
    final_columns = [
        "plant_id",
        "FuelCategory",
        "stage_code",
        "FlowName",
        "Compartment",
        "Compartment_path",
        "FlowUUID",
        "Unit",
        "ElementaryFlowPrimeContext",
        "Source",
        "Year",
    ] + actual_quant_columns
    
    if "input" in upstream_columns:
        final_columns = final_columns + ["input"]

    # I added the section below to help generate lists of matched and unmatched
    # flows. Because of the groupby, it's expensive enough not to run every
    # time and I didn't want to get rid of it in case it comes in handy later.
    if kwargs != {}:
        if "group_name" in kwargs:
            logging.info("kwarg group_name used: generating flows lists")
            upstream_df = upstream_df[[
                "FlowName_orig",
                "Compartment_path_orig",
                "stage_code"
            ]]
            unique_orig = upstream_df.groupby(
                by=["FlowName_orig", "Compartment_path_orig"]
            ).groups
            unique_mapped = upstream_mapped_df.groupby(
                by=[
                    "FlowName_orig",
                    "Compartment_path_orig",
                    "Unit_orig",
                    "FlowName",
                    "Compartment",
                    "Unit",
                    "Source"
                ]
            ).groups
            unique_mapped_set = set(unique_mapped.keys())
            unique_orig_set = set(unique_orig.keys())
            unmatched_list = sorted(list(unique_orig_set - unique_mapped_set))
            matched_list = sorted(list(unique_mapped_set))
            fname_append = f"_{kwargs['group_name']}"
            out_path = f"{output_dir}/flowmapping_lists{fname_append}.txt"
            with open(out_path, "w") as f:
                f.write("Unmatched flows\n")
                if kwargs is not None:
                    if kwargs["group_name"] is not None:
                        f.write(f"From the group: {kwargs['group_name']}\n")
                for x in unmatched_list:
                    f.write(f"{x}\n")
                f.write("\nMatched flows\n")
                for x in matched_list:
                    f.write(f"{x}\n")
                f.close()
            logging.info("Flow mapping results written to %s" % out_path)
            upstream_mapped_df = upstream_mapped_df[final_columns]

            return upstream_mapped_df, unmatched_list, matched_list

    upstream_mapped_df = upstream_mapped_df[final_columns]
    return upstream_mapped_df


def concat_clean_upstream_and_plant(pl_df, up_df):
    """Combine the upstream and the generator (power plant) databases.

    Includes some database cleanup.

    Parameters
    ----------
    pl_df : pandas.DataFrame
        The generator data frame, generated by electricitylci.generation,
        with required columns: "eGRID_ID"

    up_df : pandas.DataFrame
        The combined upstream data frame.

    Returns
    -------
    pandas.DataFrame
    """
    # Match location data to the upstream inventory
    region_cols = [
        "NERC",
        "Balancing Authority Code",
        "Balancing Authority Name",
        "Subregion",
        "FERC_Region",
        "EIA_Region",
        "State",
    ]
    existing_region_cols=[x for x in pl_df.columns if x in region_cols]

    up_df.drop(columns='eGRID_ID', errors="ignore", inplace=True)
    # 3/19/2025 MBJ reg_map eGRID_ID is int64. Setting plant_id to the same
    up_df["plant_id"]=up_df["plant_id"].astype("int64")
    reg_map = (
        pl_df[["eGRID_ID"] + existing_region_cols]
        .drop_duplicates()
        .set_index("eGRID_ID")
    )
    # 3/19/2025 MBJ: more memory management. When this process is called from
    # __init__.combine_upstream_and_gen_df the up_df is 12GB big. Previously
    # we used a merge to add all the regional columns, but that requires a 
    # tremendous amount of memory. Invidually assigning columns will be a bit
    # slower but will greatly reduce memory usage...and ultimately end up 
    # faster if your computer tends to run out of memory using the previous
    # merge.
    for col in existing_region_cols:
        up_df[col]=up_df["plant_id"].map(reg_map[col])

    # HOTFIX: during the merge, a lot eGRID_IDs are unmatched, so fill them in!
    # NOTE: triggers a pandas futurewarning on downcasting object datatypes.
    # 3/19/2025 - these would be instances where there is a plant_id in up_df
    # but not a matching eGRID_ID. With the new, by-column mapping performed above
    # eGRID_ID does not exist so no Nans to fill. In previous versions, I believe
    # the use of fillnans with plant_id being the source would result in the 
    # same thing as below.
    up_df['eGRID_ID'] = up_df['plant_id'].astype("int")

    # NOTE: the only columns in up_df not in pl_df should be:
    # 'plant_id', 'quantity', and 'input'

    # The columns in pl_df not in up_df are:
    # 'PGM_SYS_ID',
    # 'plant_name',
    # 'PrimaryFuel',
    # 'PercentGenerationfromDesignatedFuelCategory',
    # 'SourceListName'
    # 'Age'
    # 'TemporalCorrelation'

    # 3/18/25 MBJ some more changes to try reduce memory usage.
    columns_to_delete=[
        "plant_id",
        "FuelCategory_right",
        "Net Generation (MWh)",
        "PrimaryFuel_right",
        "plant_name"
    ]
    pl_df.drop(columns=columns_to_delete, errors="ignore", inplace=True)
    up_df.drop(columns=columns_to_delete, errors="ignore", inplace=True)
    # Fixing a mismatch in datatypes between up_df and pl_df that I think
    # was causing massive slow down.
    up_df["FlowAmount"]=up_df["FlowAmount"].astype(float)
    # Add plant-level data to upstream
    combined_df = pd.concat([pl_df, up_df], ignore_index=True)

    # Memory management
    del(pl_df)
    del(up_df)

    # Remove unnecessary columns
    # categories_to_delete = [
    #     "plant_id",
    #     "FuelCategory_right",
    #     "Net Generation (MWh)",
    #     "PrimaryFuel_right",
    # ]
    # categories_to_delete = [
    #     x for x in categories_to_delete if x in combined_df.columns]
    # if len(categories_to_delete) > 0:
    #     combined_df.drop(columns=categories_to_delete, inplace=True)

    combined_df["FacilityID"] = combined_df["eGRID_ID"]

    # I think without the following, given the way the data is created for
    # fuels, there are too many instances where fuel demand can be created
    # when no emissions are reported for the power plant. This should force
    # the presence of a power plant in the dataset for a fuel input to be
    # counted.
    combined_df.loc[
        ~(combined_df["stage_code"] == "Power plant"), "FuelCategory"
    ] = float("nan")

    # This allows construction impacts to be aligned to a power plant type -
    # not as important in openLCA but for analyzing results outside of openLCA.
    # Removed for Issue #150, was causing me issues downstream.
    # combined_df.loc[
    #     combined_df["FuelCategory"].str.contains("CONSTRUCTION"), "FuelCategory"
    # ] = float("nan")

    combined_df = fill_nans(combined_df, model_specs.eia_gen_year)

    # The hard-coded cutoff is a workaround for now. Changing the parameter
    # to 0 in the config file allowed the inventory to be kept for generators
    # that are now being tagged as mixed.
    generation_filter = (
        combined_df["PercentGenerationfromDesignatedFuelCategory"]
        < model_specs.min_plant_percent_generation_from_primary_fuel_category / 100
    )
    if model_specs.keep_mixed_plant_category:
        combined_df.loc[generation_filter, "FuelCategory"] = "MIXED"
        combined_df.loc[generation_filter, "PrimaryFuel"] = "Mixed Fuel Type"
    else:
        combined_df = combined_df.loc[~generation_filter]

    return combined_df


def fill_nans(
        df,
        eia_gen_year,
        key_column="FacilityID",
        target_columns=[],
        dropna=True):
    """Fills nan values for the specified target columns by using the data from
    other rows, using the key_column for matches. There is an extra step
    to fill remaining nans for the state column because the module to calculate
    transmission and distribution losses needs values in the state column to
    work.

    Parameters
    ----------
    df : dataframe
        Dataframe containing nans and at a minimum the columns key_column and
        target_columns
    key_column : str, optional
        The column to match for the data to fill target_columns, by default "FacilityID"
    target_columns : list, optional
        A list of columns with nans to fill, by default []. If empty, the function
        will use a pre-defined set of columns.
    dropna : bool, optional
        After nans are filled, drop rows that still contain nans in the
        target columns, by default True

    Returns
    -------
    dataframe: hopefully with all of the nans filled.
    """
    if not target_columns:
        target_columns = [
            "Balancing Authority Code",
            "Balancing Authority Name",
            "FuelCategory",
            "NERC",
            "PercentGenerationfromDesignatedFuelCategory",
            "eGRID_ID",
            "Subregion",
            "FERC_Region",
            "EIA_Region",
            "State",
            "Electricity",
        ]
    confirmed_target = []
    for x in target_columns:
        if x in df.columns:
            confirmed_target.append(x)
        else:
            logging.debug(f"Column {x} is not in the dataframe")
    if key_column not in df.columns:
        logging.debug(
            f"Key column '{key_column}' is not in the dataframe"
        )
        raise KeyError
    for col in confirmed_target:
        key_df = (
            df[[key_column, col]]
            .dropna()
            .drop_duplicates(subset=key_column)
            .set_index(key_column)
        )
        df.loc[df[col].isnull(), col] = df.loc[
            df[col].isnull(), key_column
        ].map(key_df[col])
    plant_ba = eia860_balancing_authority(eia_gen_year).set_index("Plant Id")
    plant_ba.index = plant_ba.index.astype(int)
    if "State" not in df.columns:
        df["State"] = float("nan")
        confirmed_target.append("State")
    df.loc[df["State"].isna(), "State"] = df.loc[
        df["State"].isna(), "eGRID_ID"].map(plant_ba["State"])
    if dropna:
        import numpy as np
        df["target_nans"]=np.where(df[confirmed_target].isnull().all(1),1,0)
        df=df.loc[(df["target_nans"]==0)|(df["Electricity"].isna()),:]
        #df.dropna(subset=confirmed_target, inplace=True, how="all")
        #df.dropna(subset=["Electricity"],inplace=True)
    return df


def map_compartment_path(df):
    emission_mapping = {
        "air": "emission/air",
        "water": "emission/water",
        "ground": "emission/ground",
        "soil": "emission/ground",
        "resource": "resource",
        "NETL database/emissions": "NETL database/emissions",
        "NETL database/resources": "NETL database/resources",
    }
    resource_mapping = {
        'water': "resource/water",
        'input': "input",  # should keep electricity as a resource
        "resource": "resource",  # hotfix geothermal resource flows
    }
    # Map resources
    df.loc[df['input'], 'Compartment_path'] = df.loc[
        df['input'], 'Compartment'].map(resource_mapping)
    # Map emissions
    df.loc[~df['input'], 'Compartment_path'] = df.loc[
        ~df['input'], 'Compartment'].map(emission_mapping)

    return df


def remove_mismatched_inventories(gen_plus_up_df):
    """This is in response to USEPA issue #282, wherein upstream inventories,
    particularly construction, are matched to facilities of a different type. An
    example of this would be a generator of type COAL that includes an input
    of solar PV construction because that facility has been reported as having
    some amount of solar PV capacity. This purpose of this function is filter
    these instances out of a given dataframe.

    Parameters
    ----------
    gen_plus_up_df : pandas.Dataframe
        A dataframe that includes a combination of upstream and generator data.

    Returns
    -------
    pandas.DataFrame
    """
    # This list only includes "Source" names that are known issues in the
    # inventories. Any source not listed here is assigned NA and is kept by
    # default. The valid FuelCategories are added as an additional column
    # to the input dataframe.
    VALID_COMBINATIONS={
        "netlsolarthermal":["SOLARTHERMAL","MIXED"],
        "netlnrelwind": ["WIND","MIXED"],
        "netlconst": ["GAS","COAL","OIL","MIXED","BIOMASS"],
        "netlnrelsolarpv" : ["SOLAR","MIXED"],
    }
    gen_plus_up_df["mapped_fuel_category"] = gen_plus_up_df["Source"].map(
        VALID_COMBINATIONS
    )
    # This list comprehension checks each row to determine whether the entry
    # in FuelCategory is in the mapped_fuel_category list assigned from above.
    # If it is not included, that index is added and will be removed in
    # subsequent lines. All rows where mapped_fuel_category is NaN are
    # automatically skipped for the filter.
    #   idx: index of the row
    #   fc: fuel category
    #   mfc: mapped fuel category from VALID_COMBINATIONS.
    rows_to_drop = [
        idx
        for idx, fc, mfc in zip(
            gen_plus_up_df.dropna(subset=["mapped_fuel_category"]).index,
            gen_plus_up_df.dropna(subset=["mapped_fuel_category"])[
                "FuelCategory"
            ],
            gen_plus_up_df["mapped_fuel_category"].dropna(),
        )
        if fc not in mfc
    ]
    # Drop the indexes with mismatches and drop the new mapped_fuel_category
    # column before returning.
    gen_plus_up_df.drop(rows_to_drop, inplace=True)
    gen_plus_up_df.drop("mapped_fuel_category", axis=1, inplace=True)

    return gen_plus_up_df


##############################################################################
# MAIN
##############################################################################
if __name__ == "__main__":
    from electricitylci.generation import create_generation_process_df
    from electricitylci.import_impacts import generate_canadian_mixes
    import electricitylci.coal_upstream as coal
    import electricitylci.natural_gas_upstream as ng
    import electricitylci.petroleum_upstream as petro
    import electricitylci.geothermal as geo
    import electricitylci.solar_upstream as solar
    import electricitylci.wind_upstream as wind
    import electricitylci.nuclear_upstream as nuke

    coal_df = coal.generate_upstream_coal(2016)
    ng_df = ng.generate_upstream_ng(2016)
    petro_df = petro.generate_petroleum_upstream(2016)
    geo_df = geo.generate_upstream_geo(2016)
    solar_df = solar.generate_upstream_solar(2016)
    wind_df = wind.generate_upstream_wind(2016)
    nuke_df = nuke.generate_upstream_nuc(2016)
    upstream_df = concat_map_upstream_databases(
        petro_df, geo_df, solar_df, wind_df, nuke_df
    )
    plant_df = create_generation_process_df()
    plant_df["stage_code"] = "Power plant"
    logging.info(plant_df.columns)
    logging.info(upstream_df.columns)
    combined_df = concat_clean_upstream_and_plant(plant_df, upstream_df)
    canadian_inventory = generate_canadian_mixes(combined_df, 2016)
    combined_df = pd.concat([combined_df, canadian_inventory])
    combined_df.sort_values(
        by=["eGRID_ID", "Compartment", "FlowName", "stage_code"], inplace=True
    )
    combined_df.to_csv(f"{output_dir}/combined_df.csv")
